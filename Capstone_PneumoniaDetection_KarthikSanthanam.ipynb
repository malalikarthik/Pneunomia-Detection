{"metadata":{"colab":{"collapsed_sections":[],"name":"Project-CNN-KarthikSanthanam.ipynb","provenance":[]},"kernelspec":{"display_name":"TensorFlow-GPU","language":"python","name":"tensorflow"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n### Submitted By: Karthik Santhanam (AIML MAY Group - 3)Â¶\n### Project: Advanced Computer Vision","metadata":{"id":"zpawPtuX_Itv"}},{"cell_type":"markdown","source":"### **PART - 1**\n\n#### DOMAIN\nEntertainment\n#### CONTEXT\nCompany X owns a movie application and repository which caters movie streaming to millions of users who on subscription basis. Company wants to automate the process of cast and crew information in each scene from a movie such that when a user pauses on the movie and clicks on cast information button, the app will show details of the actor in the scene. Company has an in-house computer vision and multimedia experts who need to detect faces from screen shots from the movie scene.\n#### DATA DESCRIPTION\nThe dataset comprises of images and its mask where there is a human face.\n#### PROJECT OBJECTIVE\nFace detection from training images.","metadata":{"id":"gJQokM_8AKAd"}},{"cell_type":"markdown","source":"### Step 1.1 - Import the dataset.","metadata":{"id":"jGmx6Bk8_FVG"}},{"cell_type":"markdown","source":"Let us imports all required packages before importing dataset","metadata":{"id":"QeTwpws1AgA5"}},{"cell_type":"code","source":"# Adding imports required for data cleansing and visualization\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes=True)\n%matplotlib inline \n\n# Adding imports for misc and data prep\nimport os\nimport cv2\n\n# to ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %tensorflow_version 2.x\nimport tensorflow\ntensorflow.__version__","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the random number generator\nimport random\nrandom.seed(0)\ntensorflow.random.set_seed(0)\n\nfrom sklearn.model_selection import train_test_split\n\n# NN Core imports\nfrom tensorflow import keras\nfrom tensorflow.keras.applications.mobilenet import preprocess_input\nfrom tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.layers import Concatenate, Conv2D, Reshape, UpSampling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.metrics import Recall, Precision","metadata":{"id":"8hisqVsPAZ_e","outputId":"086b6cf2-4ed5-419f-e6ca-6d172b32e451"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's import data set","metadata":{}},{"cell_type":"code","source":"project_path = \"./Part-1\"\ntrain_datafile = project_path + './Part 1- Train data - images.npy'\ntest_image = project_path + './Part 1Test Data - Prediction Image.jpeg'\n\ntraining_images = np.load(train_datafile, allow_pickle=True);\nprint (\"Training images (training_images) data type -> \", type (training_images))\nprint (\"Training images shape -> \", training_images.shape)\n\n# Lets store the total images. This will be required to build test/train image batches\nno_of_images = training_images.shape[0]","metadata":{"id":"r6hVvnexE6tT","outputId":"98013d57-090a-4b88-e64b-7d5aaa854508"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay.. \n- We have got a multi dimensional array as input data. \n- Let us see what we have got by printing first element of our 2-d array.","metadata":{}},{"cell_type":"code","source":"training_images[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (type(training_images[0][0])) #-- Element 0 of 1st array element\nprint (type(training_images[0][1])) #-- Element 1 of 1st array element","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above data printed for 1st element of input image data, we see that each element has,\n1. Image itself (represented in an array)\n2. Image cast information (represented by a list)","metadata":{}},{"cell_type":"markdown","source":"### Step 1.2 - Create features (images) and labels (mask) using that data.","metadata":{}},{"cell_type":"markdown","source":"Now, Let us extract image and mask information ","metadata":{}},{"cell_type":"code","source":"input_images = training_images[:,0]\ninput_masks = training_images[:,1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us visualize and understand the data that we have got !","metadata":{}},{"cell_type":"code","source":"# Let us print one test image - say 10th image\nplt.imshow(input_images[10]);\ninput_masks[10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us print another test image - say 100th image\nplt.imshow(input_images[100]);\ninput_masks[100]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n- In the first image printed (input_images[10]), we have one face and so 1st element in the list giving cast information.\n- In the second image printed (input_images[100]), we have three faces and so there are 3 elements in the list giving cast information.","metadata":{}},{"cell_type":"markdown","source":"### Step 1.3 - Mask detection model:","metadata":{}},{"cell_type":"markdown","source":"#### Step 1.3.1 - Design a face mask detection model.\nHint: Use U-net along with pre-trained transfer learning models","metadata":{}},{"cell_type":"markdown","source":"**- Defining hyper parameters -**","metadata":{}},{"cell_type":"code","source":"# Define the neccessary parameters\n\n# Width hyper parameter for MobileNet (0.25, 0.5, 0.75, 1.0). Higher width means more accurate but slower\nALPHA = 1\n\n# Ref: https://arxiv.org/pdf/1704.04861.pdf (Mobilenet paper shows that expected input image size is 224x224x3)\nIMAGE_SIZE = 224","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**- Input image pre-processing -**\n\nLet us make our training and test split vectors from input images and mask arrays","metadata":{}},{"cell_type":"code","source":"# Preparing an empty array for masks\ntraining_masks = np.zeros((no_of_images, IMAGE_SIZE, IMAGE_SIZE)) \n\n# Preparing an empty array for training images\ntraining_images = np.zeros((no_of_images, IMAGE_SIZE, IMAGE_SIZE, 3))\n\nfor index in range(no_of_images):\n    img = input_images[index]\n    img = cv2.resize(img, dsize=(IMAGE_SIZE, IMAGE_SIZE))\n    # Looks like there is one particular image with 4 channels and so an exception in resize\n    # Lets add a try catch to handle that image\n    try:\n      img = img[:, :, :3]\n    except:\n      continue\n    training_images[index] = preprocess_input(np.array(img, dtype=np.float32))\n    for i in input_masks[index]:\n        x1 = int(i[\"points\"][0]['x'] * IMAGE_SIZE)\n        x2 = int(i[\"points\"][1]['x'] * IMAGE_SIZE)\n        y1 = int(i[\"points\"][0]['y'] * IMAGE_SIZE)\n        y2 = int(i[\"points\"][1]['y'] * IMAGE_SIZE)\n        training_masks[index][y1:y2, x1:x2] = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**- Check shape of pre-processed images -**\n\nLet us check the shape (dimension) of training_images and training_masks array","metadata":{}},{"cell_type":"code","source":"print (\"Shape of training_images is -> \", training_images.shape)\nprint (\"Shape of training_masks is -> \", training_masks.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**- Let's visualize test images and masks -**","metadata":{}},{"cell_type":"code","source":"numOfRows = 6; # Number of samples to print\nnumOfCols = 2; # Two rows for display in each column. Index-0 for reference image and index-1 for related mask image\n\nf, ax = plt.subplots(nrows=numOfRows, ncols=numOfCols, figsize=(10,20))\ni=0;\nfor ro in range(4,10):\n    ax[i, 0].imshow(training_images[ro]);\n    ax[i, 1].imshow(training_masks[ro]);        \n    i=i+1;\nplt.tight_layout()\nplt.show();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**- Let's split images/masks as train/test images -**","metadata":{}},{"cell_type":"markdown","source":"- Let us have 80:20 split for train and validation set\n- Further from with in validation sample, lets split 80:20 for validation and test set","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(training_images, training_masks, test_size=0.2, random_state=1, shuffle=False)\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.2, random_state=1, shuffle=False)\n\nprint(\"Shape of X_train -> \", X_train.shape, \", Shape of y_train -> \", y_train.shape)\nprint(\"Shape of X_val -> \", X_val.shape, \", Shape of y_val -> \", y_val.shape)\nprint(\"Shape of X_test -> \", X_test.shape, \", Shape of y_test -> \", y_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**- Let's design our model -**","metadata":{}},{"cell_type":"code","source":"#To get predictable results all the times\nnp.random.seed(0)\nrandom.seed(0)\ntensorflow.random.set_seed(0)\n\ndef create_model(trainable=False):\n    model = MobileNet(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, alpha=ALPHA, weights=\"imagenet\")\n\n    for layer in model.layers:\n        layer.trainable = trainable\n        \n    block00 = model.layers[0].input\n    block01 = model.get_layer(\"conv_pw_1_relu\").output\n    block02 = model.get_layer(\"conv_pw_2_relu\").output\n    block03 = model.get_layer(\"conv_pw_3_relu\").output\n    block05 = model.get_layer(\"conv_pw_5_relu\").output\n    block11 = model.get_layer(\"conv_pw_11_relu\").output\n    block13 = model.get_layer(\"conv_pw_13_relu\").output\n\n    decoderBlock = Concatenate()([UpSampling2D()(block13), block11])\n    decoderBlock = Concatenate()([UpSampling2D()(decoderBlock), block05])\n    decoderBlock = Concatenate()([UpSampling2D()(decoderBlock), block03])\n    decoderBlock = Concatenate()([UpSampling2D()(decoderBlock), block01])\n    decoderBlock = Concatenate()([UpSampling2D()(decoderBlock), block00])\n\n    decoderBlock = Conv2D(1, kernel_size=1, activation=\"sigmoid\")(decoderBlock)\n    decoderBlock = Reshape((IMAGE_SIZE, IMAGE_SIZE))(decoderBlock)\n\n    return Model(inputs=model.input, outputs=decoderBlock) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On top of standard mobile, we have added 5 layers of skip connection + one layer of Conv2D with sigmoid activation + one layer of Reshape, in building our U-Net model.\n- Note:\n  - Mobilenet model is created with imagenet weights and we are not re-training any layer of existing mobile-net model. We are going to create MobileNet with 'imagenet' weights, so there by we are using transfer learning !\n  - We will essentially have learnable parameters for last layer of Conv2D that we have added.\n\nLet us create out model now...","metadata":{}},{"cell_type":"code","source":"#To get predictable results all the times\nnp.random.seed(0)\nrandom.seed(0)\ntensorflow.random.set_seed(0)\n\nmodel = create_model()\n\n# Print summary\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our UNet model is now ready !","metadata":{}},{"cell_type":"markdown","source":"#### Step 1.3.2 - Design your own Dice Coefficient and Loss function.","metadata":{}},{"cell_type":"markdown","source":"**Define dice coefficient**","metadata":{}},{"cell_type":"code","source":"def dice_coefficient(y_true, y_pred):\n    numerator = 2 * tensorflow.reduce_sum(y_true * y_pred)\n    denominator = tensorflow.reduce_sum(y_true + y_pred)\n    # reduce_sum(), computes the sum of elements across dimensions of a tensor.\n    # Since the 2nd arg, i.e., axis of reduce_sum() is 'None', it reduces all dimensions, \n    # and a tensor with a single element is returned.\n    \n    return numerator / (denominator + tensorflow.keras.backend.epsilon())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define Loss function**","metadata":{}},{"cell_type":"code","source":"def loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - tensorflow.keras.backend.log(dice_coefficient(y_true, y_pred) + tensorflow.keras.backend.epsilon())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Step 1.3.3 - Train, tune and test the model.","metadata":{}},{"cell_type":"markdown","source":"**- Compile the model -**\n- Define Optimizer and compile the model\n- Define Callbacks","metadata":{}},{"cell_type":"code","source":"# To get predictable results all the times\nnp.random.seed(0)\nrandom.seed(0)\ntensorflow.random.set_seed(0)\n\n# Defining optimizer and compile\noptimizer = Adam(lr=0.001)\nmodel.compile(loss=loss, optimizer=optimizer, metrics=[dice_coefficient, Recall(), Precision()])\n\n# We need not compute Recall() and Precision() in image recognition, especially when we have dice_coefficient.\n# We are still doing it as a good practice.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To get predictable results all the times\nnp.random.seed(0)\nrandom.seed(0)\ntensorflow.random.set_seed(0)\n\n# Define Callbacks\ncallbacks = [\n    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2),\n    EarlyStopping(monitor='val_loss', patience=2)\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train the Model","metadata":{}},{"cell_type":"code","source":"#To get predictable results all the times\nnp.random.seed(0)\nrandom.seed(0)\ntensorflow.random.set_seed(0)\n\nmodel.fit(\n    X_train,\n    y_train,\n    validation_data = (X_val, y_val),\n    epochs=5,\n    batch_size=32,\n    callbacks=callbacks,\n    verbose=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Step 1.3.4 - Evaluate the model using testing data.","metadata":{}},{"cell_type":"markdown","source":"Lets evaluate the model","metadata":{}},{"cell_type":"code","source":"model.evaluate(X_test, y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 1.4 - Use the âPrediction imageâ as an input to your designed model and display the output of the image.","metadata":{}},{"cell_type":"code","source":"# Test image pre-processing\ntest_image_unscaled = cv2.imread(test_image)\nimage = cv2.resize(test_image_unscaled, (IMAGE_SIZE, IMAGE_SIZE))\ntest_image_scaled = preprocess_input(np.array(image, dtype=np.float32))\nprint (\"Shape of preprocess'd image\", test_image_scaled.shape)\n\n# Test image prediction - Predict and prepare mask\n# Our predict() API generate predictions for the input samples. So add an index at 0th position for a single image prediction.\nimage_array = np.array([test_image_scaled])\nprint (\"Shape of image to be used for prediction\", image_array.shape)\n\n# Predict now\npredict_image_arr = model.predict (x=image_array)\n\n# Convert to grey scale\nunscaled_pred_mask = (predict_image_arr[0] > 0.5) \n\n# Resize predicted image\nscaled_pred_mask = cv2.resize(1.0 * unscaled_pred_mask, (IMAGE_SIZE,IMAGE_SIZE))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Original test image**","metadata":{}},{"cell_type":"code","source":"plt.imshow(test_image_unscaled);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predicted mask image**","metadata":{}},{"cell_type":"code","source":"plt.imshow(scaled_pred_mask);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Okay. Our model has detected two faces out of the test image, which is CORRECT ! :)***\n- Note: \n  - Because of time and processor/system constraints, I've just run training for 5 epocs.\n  - We can train the model for more epocs to get better prediction results.","metadata":{}},{"cell_type":"markdown","source":"### **PART - 2**\n\n#### DOMAIN\nFace recognition\n#### CONTEXT\nCompany X intends to build a face identi!ication model to recognise human faces.\n#### DATA DESCRIPTION\nThe dataset comprises of images and its mask where there is a human face.\n#### PROJECT OBJECTIVE\nFace Aligned Face Dataset from Pinterest. This dataset contains 10,770 images for 100 people. All images are taken\nfrom 'Pinterest' and aligned using dlib library.\n\n#### TASK\nIn this problem, we use a pre-trained model trained on Face recognition to recognise similar faces. Here, we are particularly interested in recognising whether two given faces are of the same person or not.","metadata":{"id":"gJQokM_8AKAd"}},{"cell_type":"markdown","source":"### Step 2.1 - Load the dataset and create the metadata","metadata":{}},{"cell_type":"code","source":"# All required imports\nfrom zipfile import ZipFile\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport cv2\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\nfrom tensorflow.keras.models import Model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**-- Setting up the project path --**","metadata":{}},{"cell_type":"code","source":"#### Setting the project path and input/test images paths####\nproject_path = \"./Part-2\"\n\ninput_zipfile = project_path + './Part 3 - Aligned Face Dataset from Pinterest.zip'\ntest_image_DwayneJohnson = project_path + './Part 2 - Test Image - Dwayne Johnson4.jpg'\ntest_image_BenedictCumberbatch = project_path + './Part 2- Test Image - Benedict Cumberbatch9.jpg'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**-- Lets extract the input zipfile --**","metadata":{}},{"cell_type":"code","source":"#### Extract the input zip file ####\nwith ZipFile(input_zipfile, 'r') as zip:\n    print('Extracting all images...')\n    zip.extractall()\n    print ('Zip File extracted !')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**-- Function to load images from extracted folder and map each image with person id --**","metadata":{}},{"cell_type":"code","source":"#### Define a function to load the images from the extracted folder and map each image with person id  ####\n\nclass IdentityMetadata():\n    def __init__(self, base, name, file):\n        # print(base, name, file)\n        # dataset base directory\n        self.base = base\n        # identity name\n        self.name = name\n        # image file name\n        self.file = file\n\n    def __repr__(self):\n        return self.image_path()\n\n    def image_path(self):\n        return os.path.join(self.base, self.name, self.file) \n    \ndef load_metadata(path):\n    metadata = []\n    for i in os.listdir(path):\n        for f in os.listdir(os.path.join(path, i)):\n            # Check file extension. Allow only jpg/jpeg' files.\n            ext = os.path.splitext(f)[1]\n            if ext == '.jpg': #Just taking jpg images and avoiding any directories or temp files in processing\n                metadata.append(IdentityMetadata(path, i, f))\n    return np.array(metadata)\n\nmetadata = load_metadata('PINS')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Number of elements in metadata -> \", len(metadata))\nprint (\"\\nShape of metadata -> \", metadata.shape)\nprint (\"\\nMetadata contents are like,\\n\", metadata[0:10])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, Good to see that the input images count is 10770, which is same as the count mentioned in the problem definition !","metadata":{}},{"cell_type":"markdown","source":"### Step 2.2 - Check some samples of metadata.","metadata":{}},{"cell_type":"markdown","source":"**-- Function to load images from metadata --**","metadata":{}},{"cell_type":"code","source":"#### Function to load an image ####\n#### Define a function to load image from the metadata ####\ndef load_image(path):\n    img = cv2.imread(path, 1)\n    # OpenCV loads images with color channels\n    # in BGR order. So we need to reverse them\n    return img[...,::-1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**-- Loading sample image --**","metadata":{}},{"cell_type":"code","source":"#### Let us load some sample images using the function \"load_image\" ####\n# Lets load some images of Shakira :)\n\nplt.figure(figsize=(10,10))\nplt.suptitle('SHAKIRA !!!')\nplt.subplot(331)\nplt.imshow(load_image('PINS\\pins_shakira\\shakira0.jpg'));\nplt.subplot(332)\nplt.imshow(load_image('PINS\\pins_shakira\\shakira1.jpg')) ;\nplt.subplot(333)\nplt.imshow(load_image('PINS\\pins_shakira\\shakira2.jpg'));\nplt.subplot(334)\nplt.imshow(load_image('PINS\\pins_shakira\\shakira3.jpg')) ;\nplt.subplot(335)\nplt.imshow(load_image('PINS\\pins_shakira\\shakira4.jpg'));\nplt.subplot(336)\nplt.imshow(load_image('PINS\\pins_shakira\\shakira5.jpg')) ;\nplt.subplot(337)\nplt.imshow(load_image('PINS\\pins_shakira\\shakira6.jpg')) ;\nplt.subplot(338)\nplt.imshow(load_image('PINS\\pins_shakira\\shakira7.jpg')) ;\nplt.subplot(339)\nplt.imshow(load_image('PINS\\pins_shakira\\shakira8.jpg')) ;","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2.3 - Load the pre-trained model and weights.","metadata":{}},{"cell_type":"markdown","source":"**-- VGG Face model --**","metadata":{}},{"cell_type":"code","source":"# Let us create the structure of VGG face model !\n\ndef vgg_face():\t\n    model = Sequential()\n    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n    model.add(Convolution2D(64, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(128, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(128, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Convolution2D(2622, (1, 1)))\n    model.add(Flatten())\n    model.add(Activation('softmax'))\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**-- Load the model and given weight file named \"vgg_face_weights.h5\" --**","metadata":{}},{"cell_type":"code","source":"model = vgg_face()\nmodel.load_weights(project_path + './Part 3 - vgg_face_weights.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**-- Get vgg_face_descriptor --**","metadata":{}},{"cell_type":"code","source":"vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2.4 - Generate Embedding vectors for each face in the dataset","metadata":{}},{"cell_type":"markdown","source":"**-- Generate embeddings for each image in the dataset (list of images in metadata) --**","metadata":{}},{"cell_type":"code","source":"## Logic: \n## 1. Iterate through metadata (list with input image filenames)\n## 2. Load the image and create embeddings for each image using vgg_face_descriptor.predict() \n## 3. Store embeddings for each input image in a list by name embeddings\n## Note: Final embedding from the model for each image is of size 2622 (Based on last Convolution2D layer in the model)\n\nembeddings = np.zeros((metadata.shape[0], 2622))\n\nfor i, m in enumerate(metadata):\n    img_path = metadata[i].image_path()\n    img = load_image(img_path)\n    img = (img / 255.).astype(np.float32)\n    img = cv2.resize(img, dsize = (224,224))\n    embedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]\n    embeddings[i]=embedding_vector\nprint('embeddings shape :', embeddings.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us have a look at sample embeddings value for input images with,\n- index 0 (first image), \n- index 1 (second image) and \n- index 10769 (last image)","metadata":{}},{"cell_type":"code","source":"print (\"Embeddings for image in index-0 of shape\", embeddings[0].shape, \" is,\\n\", embeddings[0])\nprint (\"Embeddings for image in index-1 of shape\", embeddings[1].shape, \" is,\\n\", embeddings[1])\nprint (\"Embeddings for image in index-10769 of shape\", embeddings[10769].shape, \" is,\\n\", embeddings[10769])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2.5 - Build distance metrics for identifying the distance between two given images","metadata":{}},{"cell_type":"markdown","source":"**-- Function to calculate distance between given 2 pairs of images --**\n- Consider distance metric as \"Squared L2 distance\"\n- Squared l2 distance between 2 points (x1, y1) and (x2, y2) = (x1-x2)^2 + (y1-y2)^2","metadata":{}},{"cell_type":"code","source":"def distance(emb1, emb2):\n    return np.sum(np.square(emb1 - emb2))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us write a function called \"show_pair(image1, image2)\" that will,\n- Take 2 images as input\n- Calls distance() function and\n- Plots the images and shows the distance","metadata":{}},{"cell_type":"code","source":"def show_pair(idx1, idx2):\n    plt.figure(figsize=(8,3))\n    plt.suptitle(f'Distance = {distance(embeddings[idx1], embeddings[idx2]):.2f}')\n    plt.subplot(121)\n    plt.imshow(load_image(metadata[idx1].image_path()))\n    plt.subplot(122)\n    plt.imshow(load_image(metadata[idx2].image_path()));    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets call show_pair() function with one 'Positive pair' and 'Negative pair'","metadata":{}},{"cell_type":"code","source":"show_pair(1, 2) # Positive pair (Distance value will be less)\nshow_pair(1, 200) # Negative pair (Distance value will be more)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**-- Create train and test sets --**\n- 90% of input data is training data and\n- 10% of input data is test data","metadata":{}},{"cell_type":"code","source":"# Every 9th element in metadata goes to test data and rest goes into train data\ntrain_idx = np.arange(metadata.shape[0]) % 9 != 0     \ntest_idx = np.arange(metadata.shape[0]) % 9 == 0\n\n# Assign train_idx images to X_train, test_idx images to X_test\nX_train = embeddings[train_idx]\nX_test = embeddings[test_idx]\n\n# Get the target image names from metadata for x_train (with train_idx) and x_test (test_idx). \n# Put the same in y_train and y_test\ntargets = np.array([m.name for m in metadata])\ny_train = targets[train_idx]\ny_test = targets[test_idx]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets check the shape of train and test images.","metadata":{}},{"cell_type":"code","source":"print('X_train shape -> ', X_train.shape[0], X_train.shape[1])\nprint('y_train shape -> ', y_train.shape[0])\nprint('X_test shape -> ', X_test.shape[0], X_test.shape[1])\nprint('y_test shape -> ', y_test.shape[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(np.unique(y_test)), len(np.unique(y_train))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Above data shows that we have all 100 categories of face images in both train and test data set. \n- We are all set good !","metadata":{}},{"cell_type":"markdown","source":"**-- Let us encode the target labels, using LabelEncoder --**","metadata":{}},{"cell_type":"code","source":"# Create an object of LabelEncoder\nle = LabelEncoder()\n\n# Do fit_transform of target values of train data\ny_train_encoded = le.fit_transform(y_train)\n\n# Do fit_transform of target values of test data\ny_test_encoded = le.fit_transform(y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us print and understand how LabelEncoder has put our target data","metadata":{}},{"cell_type":"code","source":"# Class names (we must have 100 categories here)\nprint(\"Number of target class types is -> \", len(le.classes_))\nprint(\"\\n Target classes are\\n\", le.classes_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us print the encoded y_train and y_test data\nprint('y_train_encoded : ', y_train_encoded)\nprint('y_test_encoded : ', y_test_encoded)\nlen(np.unique(y_train_encoded)), len(np.unique(y_test_encoded))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay.. So we have got all 100 classes/categories starting from 0 to 99 encoded values, in each target (train and test)","metadata":{}},{"cell_type":"markdown","source":"**-- Standardize the input feature values, using StandardScaler --**","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_std = scaler.transform(X_train)\nX_test_std = scaler.transform(X_test)\n\nprint('X_train_std shape -> ', X_train_std.shape)\nprint('y_train_encoded shape -> ', y_train_encoded.shape)\nprint('X_test_std shape -> ', X_test_std.shape)\nprint('y_test_encoded shape -> ', y_test_encoded.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2.6 - Use PCA for dimensionality reduction\n\n- Reduce feature dimensions using Principal Component Analysis\n- Set the parameter n_components=128","metadata":{}},{"cell_type":"code","source":"# Let us reduce the input feature dimension from size 2622 to 128 !\npca = PCA(n_components=128)\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\n\nprint('X_train shape before PCA -> ', X_train_std.shape)\nprint('X_test shape before PCA -> ', X_test_std.shape)\n\nprint('X_train shape after PCA -> ', X_train_pca.shape)\nprint('X_test shape after PCA -> ', X_test_pca.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay. So we now see that feature embedding size has come down significantly from 2622 to 128 as part of PCA ! ","metadata":{}},{"cell_type":"markdown","source":"### Step 2.7 - Build an SVM classifier in order to map each image to its right person.\n\n- Use SVM Classifier to predict the person in the given image\n- Fit the classifier and print the score","metadata":{}},{"cell_type":"code","source":"clf = SVC(kernel='rbf', gamma='scale', random_state=1)\nclf.fit(X_train_pca, y_train_encoded)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us evaluate how good is our model with test data verifications !","metadata":{}},{"cell_type":"code","source":"y_predict = clf.predict(X_test_pca)\nprint('y_predict of shape', y_predict.shape, ' : ',y_predict)\nprint('y_test_encoded of shape', y_test_encoded.shape, ' : ',y_test_encoded)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us score model performance !","metadata":{}},{"cell_type":"code","source":"accuracy_score(y_test_encoded, y_predict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2.8 - Import the the test image. Display the image. Use the SVM trained model to predict the face.\n\n1. Import test images\n2. Display test images\n3. Use trained SVM model to predict the face (class of images)","metadata":{}},{"cell_type":"code","source":"def predict_person(test_input_image, debug=False):\n    # Load the image\n    raw_image = load_image(test_input_image)\n\n    # Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\n    raw_image_normalized = (raw_image / 255.).astype(np.float32)\n    raw_image_normalized_resized = cv2.resize(raw_image_normalized, dsize = (224,224))\n    if (debug):\n        print(\"Shape of normalized and resized image is\", raw_image_normalized_resized.shape)\n\n    # Get the embedding vector for the above image using vgg_face_descriptor model and print the shape \n    embedding_vector = vgg_face_descriptor.predict(np.expand_dims(raw_image_normalized_resized, axis=0))[0]\n    if (debug):\n        print(\"Shape of embedding vector\", embedding_vector.shape)\n\n    embedding_vector = np.expand_dims(embedding_vector, axis=0)\n    if (debug):\n        print(\"Shape of embedding vector\", embedding_vector.shape)\n\n    # Standarize features\n    embedding_vector_std = scaler.transform(embedding_vector)\n    if (debug):\n        print('embedding_vector_std shape : ({0},{1})'.format(embedding_vector_std.shape[0], embedding_vector_std.shape[1]))\n\n    # Reduce the input feature dimension from size 2622 to 128 !\n    embedding_vector_pca = pca.transform(embedding_vector_std)\n\n    # Run SVC classifier to predict the output class\n    predicted_class_vector = clf.predict(embedding_vector_pca)\n\n    # Lets get the class name from predicted class vector\n    predicted_class_name = le.inverse_transform(predicted_class_vector)\n\n    plt.imshow(raw_image_normalized_resized)\n    plt.title(f'Person identified as -> {predicted_class_name}');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Running prediction on 1st test image","metadata":{}},{"cell_type":"code","source":"predict_person(test_image_DwayneJohnson, True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Running prediction on 2nd test image","metadata":{}},{"cell_type":"code","source":"predict_person(test_image_BenedictCumberbatch)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hurrey ! We are able to predict both test images successfully ! :)**","metadata":{}}]}